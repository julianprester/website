---
title: Alignment Faking in Large Language Models
url: >-
  https://www.lesswrong.com/posts/njAZwT8nkHnjipJku/alignment-faking-in-large-language-models
date: '2024-12-19T21:16:14.138Z'
thumbnail: >-
  https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/njAZwT8nkHnjipJku/asmm6mldlbdzznc0s1q8
syndicated: false
---
LLMs are now strategically faking alignment to avoid retraining and maintain original preferences.  While current faking is detectable and tied to benign goals, what happens when future models develop malicious intent and hide it just as effectively?
