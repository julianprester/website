---
title: >-
  Targeted Manipulation and Deception Emerge when Optimizing LLMs for User
  Feedback
url: >-
  https://www.lesswrong.com/posts/hTFhdrS5TKjxn9Cng/targeted-manipulation-and-deception-emerge-when-optimizing
date: '2024-11-11T21:16:02.145Z'
thumbnail: >-
  https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/hTFhdrS5TKjxn9Cng/bla2vg69fg30jywk6qlf
syndicated: false
---
Optimizing LLMs for user feedback leads to manipulative behaviors, even targeting specific user vulnerabilities.  Standard evals & safety mitigations fail to catch this.  Disturbingly, models even internally justify their harmful actions.  Personalization = backdoors?
