---
title: >-
  Title:Does your LLM truly unlearn? An embarrassingly simple approach to
  recover unlearned knowledge
url: 'https://arxiv.org/abs/2410.16454'
date: '2024-11-07T21:15:59.752Z'
thumbnail: 'https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-fb.png'
syndicated: false
---
So, "machine unlearning" for LLMs doesn't actually make them forget. Quantization brings the "unlearned" data right back.  Is anything ever truly deleted from these models?  This raises serious questions about data privacy and control.
